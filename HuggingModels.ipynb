{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8_6ddjCCI5z"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IipG1U_uh_JL"
      },
      "source": [
        "# Using Hugging Face to Run models\n",
        "\n",
        "We don't have much time, so here are our goals today:\n",
        "\n",
        "- Try some new models and some new prompts\n",
        "- Llama, GPT, and Deepseek, Open Source Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHJKNIkv-Qnj",
        "outputId": "416ad397-27d1-4d25-b487-0f5de0f60a43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `For stuyai Meeting` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `For stuyai Meeting`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HFlMQnci-9j"
      },
      "source": [
        "## Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Azz-bP5fCtUX",
        "outputId": "37ebd58b-fe22-487b-e19d-ae97430a249d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr9_bQX-EAth",
        "outputId": "b802a292-4953-44cb-b4c2-dc79eb62ab88"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once upon a time: The Story of the Belfry\n",
            "In 2010, the Belfry opened its doors to the public. A new era in the history of the castle began. The Belfry is the largest castle in the Netherlands and the most visited castle in the Netherlands. The Belfry is a real attraction for both tourists and locals. The Belfry is a castle that has been fully restored to its former glory. The Belfry is a place of history and culture.\n",
            "The Belfry is located in the city center of Utrecht. The Belfry is a beautiful castle with a rich history. The Belfry is the oldest building in the city of Utrecht. The Belfry is a place of beauty and history. The Belfry is a place of culture and art. The Belfry is a place of tradition and history.\n",
            "The Belfry is a place of beauty and history. The Belfry is a place of culture and art. The Belfry is a place of tradition and history. The Belfry is a place of beauty and history. The Belfry is a place of culture and art. The Belfry is a place of tradition and history.\n",
            "The Belfry is a place of beauty and history. The Belfry is a place of culture and art. The Belfry is a place of tradition and history. The Belfry is a place of beauty and\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Once upon a time:\"\n",
        "output = pipe(prompt, max_length=300, num_return_sequences=1)\n",
        "print(output[0]['generated_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grHz3P-di9Je"
      },
      "source": [
        "## GPT 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "6d6cd1b29fee4df784215182d852ccc4",
            "0bea9dca3cfb4c1391e43f2d666548b4",
            "02dc6258e78c4ae29f419609d2c331c4",
            "37e7059ba6584f79bddd84c908841261",
            "85bb594280a6426993fbfaea47c08bd9",
            "a02f0b9f393a4ac5bfc491628c634c32",
            "dfe2f1d7c9c34737b00b4d397074c0da"
          ]
        },
        "id": "4RuYdKozAyyK",
        "outputId": "c2c920c9-479e-4ef1-d9df-9d11192fa376"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d6cd1b29fee4df784215182d852ccc4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bea9dca3cfb4c1391e43f2d666548b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02dc6258e78c4ae29f419609d2c331c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37e7059ba6584f79bddd84c908841261",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85bb594280a6426993fbfaea47c08bd9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a02f0b9f393a4ac5bfc491628c634c32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfe2f1d7c9c34737b00b4d397074c0da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "# use GPT2, which is free to use\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66f7OiXCAzKn",
        "outputId": "4a4b89c1-0c3a-4142-8ed2-58a2f91bddae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once upon a time he was a member of the British Army who had been recruited to join the British Army. According to The Times, on 18 October 1965, The Prince's wife, Mary Jane, met the Duke of Norfolk, in London. Their children became members of the Royal Family and became the Princesses of Grenada. Although she claimed to love herself, her mother and the Duke were never seen smiling after the prince joined the Royal Army. At Prince's time he had played a key role as leader of the RAF's Bomber Command during Vietnam. He had joined in December 1967 with the Royal Flying Corps, but was transferred to RAF Luton after a number of accidents. After leaving RAF Luton, he worked as a sales rep in Glasgow, the seat of RAF Bessie. On September 20, 1968 he went to work as a customer service officer for The Telegraph's London office. The paper described him as \"the epitome of British patriotism in a position of authority: a gentleman and very good salesman\" [1]. On November 30, 1969 his wife met with the Duke, on which he took part in the \"Etore D'Or at St Joseph's\" dinner. They also became close. At the dinner he offered three biscuits which his wife bought for his money. The Duke had no idea of the price until dinner time. In the next meeting they had lunch together and later the dinner turned into a series of phone calls concerning his involvement. As the King was\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Once upon a time:\"\n",
        "output = generator(prompt, max_length=300, num_return_sequences=1)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liro7cknjCZ5"
      },
      "source": [
        "## Deepseek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgNuCBNaA1zw",
        "outputId": "118f2484-1281-44ae-91ad-ef1d87f253ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "pipe = pipeline(\"text-generation\", model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", max_length=150, num_return_sequences=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tUPq636BQWl",
        "outputId": "04f2106a-fac7-44e3-b32c-4cfa73b5482f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Once upon a time, in a land far, far away, there was a kingdom known as the **Royal Court**. The court was surrounded by a magical forest called the **Enchanted Grove**, which was said to be home to the **Enchanted Trees**. The trees were said to be the source of power and wisdom, and their magic was said to be used for both good and evil.\n",
            "\n",
            "The court was ruled by the **Royal Emperor**, who was the head of state and was said to be wise and just. The Emperor was known for his love of tea and his love of music, and he was said to be one of the most beautiful people in the kingdom\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Once upon a time:\"},\n",
        "]\n",
        "pipe(messages,max_length=300, num_return_sequences=1)\n",
        "print(pipe(messages)[0][\"generated_text\"][1][\"content\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}